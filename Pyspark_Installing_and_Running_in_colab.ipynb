{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Pyspark Installing and Running in colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-cNaWxV7WT7"
      },
      "source": [
        "\r\n",
        "Checking Java Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqchUGq9OhqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da754cd-9d67-41a1-8a08-1489af92e8a3"
      },
      "source": [
        "!java -version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"11.0.9.1\" 2020-11-04\n",
            "OpenJDK Runtime Environment (build 11.0.9.1+1-Ubuntu-0ubuntu1.18.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.9.1+1-Ubuntu-0ubuntu1.18.04, mixed mode, sharing)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj8jb90O7aLd"
      },
      "source": [
        "Setting Java 8 environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBlXGEgf7UXB"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33CTtMhA-hle"
      },
      "source": [
        "Downloading Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxp1HCHK-ud4"
      },
      "source": [
        "!wget -q http://apache.osuosl.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czSBOScm7fEB"
      },
      "source": [
        "Extracting Spark Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt5ooZ6x7c-6"
      },
      "source": [
        "!tar xf spark-3.0.1-bin-hadoop3.2.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE7d45Ec7iEQ"
      },
      "source": [
        "Installing FindSpark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URTQlaUN7hLg"
      },
      "source": [
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSu3EGrg7m7g"
      },
      "source": [
        " JVM folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCDjw03D7mJ6",
        "outputId": "7aa16fef-71f6-4b9b-bcd5-222d002580e6"
      },
      "source": [
        "!ls /usr/lib/jvm/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "default-java\t\t   java-11-openjdk-amd64     java-8-openjdk-amd64\n",
            "java-1.11.0-openjdk-amd64  java-1.8.0-openjdk-amd64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSbkzX8q756N"
      },
      "source": [
        "Pyarrow\r\n",
        "- Pyarrow is a library for building data frame internals (and other data processing applications). \r\n",
        "- It is not an end user library like pandas.\r\n",
        "- PyArrow library provides a Python API for the functionality provided by the Arrow libraries, along with tools for Arrow integration and interoperability with pandas, NumPy, and other software in the Python ecosystem.\r\n",
        "\r\n",
        "Installing Pyarrow\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq6UOuyt737d",
        "outputId": "53945832-e058-48c3-9530-3d4140a7d15d"
      },
      "source": [
        "!pip install -U pyarrow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pyarrow in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV-oc9789Rsc"
      },
      "source": [
        "Setting up Home environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0v5-EZW9KH2"
      },
      "source": [
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESks-Jpk9WJV"
      },
      "source": [
        "Creating Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t99w_qH99TmO"
      },
      "source": [
        "import findspark\r\n",
        "findspark.init()\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSpMcgEHCncy"
      },
      "source": [
        "Stopping the session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL7e72dW9aMF"
      },
      "source": [
        "#spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6RBu0SzCrux"
      },
      "source": [
        "Check the pyspark version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni_-woXpAhhs",
        "outputId": "09020d34-f925-47a3-d3d2-60056efb2a8a"
      },
      "source": [
        "import pyspark\r\n",
        "print(pyspark.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNwH7jKLCxgX"
      },
      "source": [
        "Importing Pyspark\r\n",
        "- Creating conf and it's object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Hxue3NwCvP2",
        "outputId": "c7ebb35a-5788-465d-8d98-4bd2439b31ef"
      },
      "source": [
        "from pyspark import SparkConf\r\n",
        "from pyspark import SparkContext\r\n",
        "conf = SparkConf()\r\n",
        "conf.setMaster('local')\r\n",
        "conf.setAppName('spark-basis')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.conf.SparkConf at 0x7f1848414518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2hjINUlC0_B",
        "outputId": "e1d22c90-267a-4671-a02e-272d1881bf94"
      },
      "source": [
        "sc.getConf().getAll()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.master', 'local'),\n",
              " ('spark.app.id', 'local-1610647032541'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.executor.memory', '4g'),\n",
              " ('spark.app.name', 'spark-basis'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.submit.pyFiles', ''),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.driver.port', '36003'),\n",
              " ('spark.ui.showConsoleProgress', 'true'),\n",
              " ('spark.driver.host', 'a65bcac3d49a')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4Zw8q8ODamy"
      },
      "source": [
        "We can create the Spark Context using that configuration object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQuI5XHSC66t"
      },
      "source": [
        "config = pyspark.SparkConf().setAll([('spark.executor.memory', '4g'), ('spark.driver.memory','4g'), ('spark.memory.fraction', '0.9')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlapULdrEJZM"
      },
      "source": [
        "sys \r\n",
        "- Access system-specific parameters and functions.\r\n",
        "\r\n",
        "tempfile\r\n",
        "- Module creates temporary files and directories. It works on all supported \r\n",
        "platforms. \r\n",
        "\r\n",
        "urllib\r\n",
        "- Module that can be used for opening URLs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5WcHHbTDUA6"
      },
      "source": [
        "import sys, tempfile, urllib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5vLJvijQowa"
      },
      "source": [
        "Creating a base_directory and output_file to store data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oog5T0EFQKLE"
      },
      "source": [
        "BASE_DIR ='/tmp'\r\n",
        "OUTPUT_FILE = os.path.join(BASE_DIR, 'autos_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA6zlN2PQvvG"
      },
      "source": [
        "Downloading the data from uci repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-KWxi7KQnb0"
      },
      "source": [
        "autos_data = urllib.request.urlretrieve('https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data',OUTPUT_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5JVY4FORaxj",
        "outputId": "957ef3a1-7e2f-4266-96e0-060613b04a97"
      },
      "source": [
        "!ls /tmp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "autos_data.csv\n",
            "blockmgr-7ab72995-0857-4805-9291-2e2443f76e79\n",
            "credit_data.csv\n",
            "dap_multiplexer.a65bcac3d49a.root.log.INFO.20210114-172326.51\n",
            "dap_multiplexer.INFO\n",
            "debugger_29cke107i9\n",
            "hsperfdata_root\n",
            "initgoogle_syslog_dir.0\n",
            "liblz4-java-6750267683785321441.so\n",
            "liblz4-java-6750267683785321441.so.lck\n",
            "spark-3cf3a23b-7244-4929-8be1-76ee8734cf27\n",
            "spark-a12e9b4b-0bb8-4478-82ef-dfdb30c1e044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJsD8OF8Ry6y"
      },
      "source": [
        "inferSchema \r\n",
        "- By setting inferSchema=true , Spark will automatically go through the csv file and infer the schema of each column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3Zspg8RcOTL"
      },
      "source": [
        "Importing the SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3T3RALibuKD"
      },
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB-DVtHuca7t"
      },
      "source": [
        "Initialize SparkSession object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuHYXUSBa04n"
      },
      "source": [
        "spark = SparkSession \\\r\n",
        "    .builder \\\r\n",
        "    .master(\"local\") \\\r\n",
        "    .appName(\"Spark CSV Reader\") \\\r\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnsSsS9bcfOZ"
      },
      "source": [
        "Reading the CSV file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSDn6bHwRcSm"
      },
      "source": [
        "autos_df = spark.read.option(\"inferSchema\", \"true\").csv(\"/tmp/autos_data.csv\", header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHSSfUCvciME"
      },
      "source": [
        "Showing the CSV file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cyCBBrdSE4U",
        "outputId": "f9f85207-0513-4ce1-ed21-5f4e2154ab2a"
      },
      "source": [
        "credit_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+----+---+-----+----+-----------+----+-----+------+----+-----+-----+----+----+-----+\n",
            "|_c0|_c1| _c2|_c3|  _c4| _c5|        _c6| _c7|  _c8|   _c9|_c10| _c11| _c12|_c13|_c14| _c15|\n",
            "+---+---+----+---+-----+----+-----------+----+-----+------+----+-----+-----+----+----+-----+\n",
            "|  3|  ?|null|gas|  std| two|convertible|null|front| 88.60|null|64.10|48.80|2548|null| four|\n",
            "|  3|  ?|null|gas|  std| two|convertible|null|front| 88.60|null|64.10|48.80|2548|null| four|\n",
            "|  1|  ?|null|gas|  std| two|  hatchback|null|front| 94.50|null|65.50|52.40|2823|null|  six|\n",
            "|  2|164|null|gas|  std|four|      sedan|null|front| 99.80|null|66.20|54.30|2337|null| four|\n",
            "|  2|164|null|gas|  std|four|      sedan|null|front| 99.40|null|66.40|54.30|2824|null| five|\n",
            "|  2|  ?|null|gas|  std| two|      sedan|null|front| 99.80|null|66.30|53.10|2507|null| five|\n",
            "|  1|158|null|gas|  std|four|      sedan|null|front|105.80|null|71.40|55.70|2844|null| five|\n",
            "|  1|  ?|null|gas|  std|four|      wagon|null|front|105.80|null|71.40|55.70|2954|null| five|\n",
            "|  1|158|null|gas|turbo|four|      sedan|null|front|105.80|null|71.40|55.90|3086|null| five|\n",
            "|  0|  ?|null|gas|turbo| two|  hatchback|null|front| 99.50|null|67.90|52.00|3053|null| five|\n",
            "|  2|192|null|gas|  std| two|      sedan|null|front|101.20|null|64.80|54.30|2395|null| four|\n",
            "|  0|192|null|gas|  std|four|      sedan|null|front|101.20|null|64.80|54.30|2395|null| four|\n",
            "|  0|188|null|gas|  std| two|      sedan|null|front|101.20|null|64.80|54.30|2710|null|  six|\n",
            "|  0|188|null|gas|  std|four|      sedan|null|front|101.20|null|64.80|54.30|2765|null|  six|\n",
            "|  1|  ?|null|gas|  std|four|      sedan|null|front|103.50|null|66.90|55.70|3055|null|  six|\n",
            "|  0|  ?|null|gas|  std|four|      sedan|null|front|103.50|null|66.90|55.70|3230|null|  six|\n",
            "|  0|  ?|null|gas|  std| two|      sedan|null|front|103.50|null|67.90|53.70|3380|null|  six|\n",
            "|  0|  ?|null|gas|  std|four|      sedan|null|front|110.00|null|70.90|56.30|3505|null|  six|\n",
            "|  2|121|null|gas|  std| two|  hatchback|null|front| 88.40|null|60.30|53.20|1488|null|three|\n",
            "|  1| 98|null|gas|  std| two|  hatchback|null|front| 94.50|null|63.60|52.00|1874|null| four|\n",
            "+---+---+----+---+-----+----+-----------+----+-----+------+----+-----+-----+----+----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knGHRMj2czBl"
      },
      "source": [
        "Counting the numbers of Rows in the File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxxeQK4jSorK",
        "outputId": "a7b3f568-a485-41f4-fb17-bc4a7b0ebe68"
      },
      "source": [
        "credit_df.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "205"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDdEreDvenJY"
      },
      "source": [
        "# COMPLETED"
      ]
    }
  ]
}